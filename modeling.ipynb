{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "1. This notebook implements 3 models\n",
    "    *   A lineal regression model as baseline\n",
    "    *   A tree-based as potential candidate and also as a pipeline to do Feature importance\n",
    "    *   A MLP as to use as the main regression model|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test datasets preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28590/3990379248.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  high_value_homes['cluster'] = kmeans.fit_predict(high_value_homes[['latitude', 'longitude']])\n",
      "/tmp/ipykernel_28590/3990379248.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  high_value_homes['cluster'] = kmeans.fit_predict(high_value_homes[['latitude', 'longitude']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14448, 25)\n",
      "(6192, 25)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.centroids = None\n",
    "\n",
    "\n",
    "    def __compute_distance(self, point, centroid):\n",
    "        return np.sqrt((point[0] - centroid[0])**2 + (point[1] - centroid[1])**2)\n",
    "\n",
    "    def __distance_to_closest_centroid(self, row):\n",
    "        distances = [self.__compute_distance([row['latitude'], row['longitude']], centroid) for centroid in self.centroids]\n",
    "        return min(distances)\n",
    "\n",
    "    def __get_distance_to_high_value_area(self, X):\n",
    "        if self.centroids is None:\n",
    "            raise ValueError('Fit the model first before transforming the data')\n",
    "        \n",
    "        X['distance_to_high_value_area'] = X.apply(lambda row: self.__distance_to_closest_centroid(row), axis=1)\n",
    "        return X\n",
    "\n",
    "    # Fit the KMeans clustering model for the distance to high value area feature\n",
    "    def fit(self, X, y=None, num_clusters=2):\n",
    "        threshold = X['median_house_value'].quantile(0.80)\n",
    "        high_value_homes = X[X['median_house_value'] > threshold]\n",
    "\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=10)\n",
    "        high_value_homes['cluster'] = kmeans.fit_predict(high_value_homes[['latitude', 'longitude']])\n",
    "        self.centroids = kmeans.cluster_centers_\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['bedrooms_per_room'] = X['total_bedrooms'] / X['total_rooms']\n",
    "        X['population_per_household'] = X['population'] / X['households']\n",
    "        X['rooms_per_household'] = X['total_rooms'] / X['households']\n",
    "        X['income_per_household'] = X['median_income'] / X['households']\n",
    "        X['income_per_bedroom'] = X['median_income'] / X['total_bedrooms']\n",
    "        X['income_per_room'] = X['median_income'] / X['total_rooms']\n",
    "        X['age_of_population'] = X['housing_median_age'] / X['population']\n",
    "        X['age_of_households'] = X['housing_median_age'] / X['households']\n",
    "        X['population_density'] = X['population'] / X['total_rooms']\n",
    "        X['bedrooms_per_population'] = X['total_bedrooms'] / X['population']\n",
    "        X['is_capped'] = X['median_house_value'] == 500000\n",
    "        \n",
    "\n",
    "        self.__get_distance_to_high_value_area(X)\n",
    "        \n",
    "        return X\n",
    "\n",
    "# Custom transformer to drop a column    \n",
    "class DropColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.column_name in X.columns:\n",
    "            X = X.drop(columns=[self.column_name])\n",
    "        return X\n",
    "    \n",
    "housing_data = pd.read_csv(\"data/housing.csv\")\n",
    "train_set, test_set = train_test_split(housing_data, test_size=0.3, random_state=10)\n",
    "\n",
    "# Keep the original labels for training and testing\n",
    "y_train = train_set['median_house_value'].to_numpy()\n",
    "y_test = test_set['median_house_value'].to_numpy()\n",
    "\n",
    "num_attribs = train_set.columns.tolist()\n",
    "num_attribs.remove('ocean_proximity')\n",
    "cat_attribs = ['ocean_proximity']\n",
    "\n",
    "feature_eng = FeatureEngineering()\n",
    "feature_eng.fit(train_set)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('feature_eng', feature_eng),\n",
    "    ('drop_column', DropColumnTransformer('median_house_value')),\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attribs),\n",
    "    ('cat', OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(train_set)\n",
    "X_test = pipeline.transform(test_set)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test a linear regression model\n",
    "\n",
    "To evaluate:\n",
    "1. The Root Mean Squared Error is used for the training error \n",
    "2. N-fold cross validation with RMSE is used, with 10 folds for the validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 65512.19687815971\n",
      "Linear Regression RMSE Scores: [65491.56395045 62939.19496873 64423.00782818 65956.13642558\n",
      " 67187.4882399  66054.65590843 64872.79467416 79491.74134393\n",
      " 65726.6694338  70087.43228025]\n",
      "Linear Regression RMSE Mean: 67223.06850533944\n",
      "Linear Regression RMSE Standard Deviation: 4457.011346877264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "lin_predictions = lin_reg.predict(X_train)\n",
    "\n",
    "lin_rmse = np.sqrt(mean_squared_error(y_train, lin_predictions))\n",
    "lin_scores = cross_val_score(lin_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "\n",
    "print('Linear Regression RMSE:', lin_rmse)\n",
    "print('Linear Regression RMSE Scores:', lin_rmse_scores)\n",
    "print('Linear Regression RMSE Mean:', lin_rmse_scores.mean())\n",
    "print('Linear Regression RMSE Standard Deviation:', lin_rmse_scores.std())    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note:\n",
    "\n",
    "1. It seems that the model is underfitting as the error is around $67k\n",
    "2. We could try to:\n",
    "    * Use a more powerful model\n",
    "    * Add more features that give more signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.0\n",
      "Scores: [67342.37070237 64333.83323831 69251.34131217 65383.86508618\n",
      " 70360.5906065  66883.83561217 65856.91159433 66622.5999637\n",
      " 67069.65300694 65451.15859048]\n",
      "Mean: 66855.61597131658\n",
      "Standard Deviation: 1727.9332081551922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X_train, y_train)\n",
    "tree_pred = tree_reg.predict(X_train)\n",
    "\n",
    "tree_rmse = np.sqrt(mean_squared_error(y_train, tree_pred))\n",
    "scores = cross_val_score(tree_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(f\"RMSE: {tree_rmse}\")\n",
    "print(f\"Scores: {tree_rmse_scores}\")\n",
    "print(f\"Mean: {tree_rmse_scores.mean()}\")\n",
    "print(f\"Standard Deviation: {tree_rmse_scores.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note:\n",
    "\n",
    "1. Decision tree regressor is overfitting the data (Training RMSE = 0)\n",
    "2. Performance is a little bit worse than the lineal regressor model when cross-validated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 17607.54218024274\n",
      "Scores: [46340.26170842 44399.81293542 47564.20915831 46293.68816815\n",
      " 48362.21875891 48347.57464029 46158.84466152 48441.52319964\n",
      " 46878.14361248 49678.66839416]\n",
      "Mean: 47246.494523731606\n",
      "Standard Deviation: 1450.9680355512453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(X_train, y_train)\n",
    "forest_pred = forest_reg.predict(X_train)\n",
    "\n",
    "forest_rmse = np.sqrt(mean_squared_error(y_train, forest_pred))\n",
    "scores = cross_val_score(forest_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n",
    "forest_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(f\"RMSE: {forest_rmse}\")\n",
    "print(f\"Scores: {forest_rmse_scores}\")\n",
    "print(f\"Mean: {forest_rmse_scores.mean()}\")\n",
    "print(f\"Standard Deviation: {forest_rmse_scores.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note:\n",
    "\n",
    "1. The model stills overfit a lot but given that random forest is an ensemble mechanism, this helps to generalize and regularize, the overfitting is not as bad as the decision tree\n",
    "2. We could limit the 'max_depth', 'min_samples_split' and 'min_samples_leaf' to force the model to limit the complexity of the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layered Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n",
      "RMSE: 33557.09451622558\n",
      "Scores: [45475.11838989 50040.79888641 45579.42498517 44773.79124532\n",
      " 47554.97488784 50931.20698013 45988.3425077  48157.77281182\n",
      " 45890.09238523 47036.40736751]\n",
      "Mean: 47142.79304470196\n",
      "Standard Deviation: 1941.6837196220183\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'{device=}')\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "\n",
    "# Custom wrapper class to use PyTorch model with scikit-learn cross-validation\n",
    "class PyTorchRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model, epochs=100, lr=0.01):\n",
    "        self.model = model.to(device)\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        self.model.train()\n",
    "        for _ in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X).cpu().numpy()\n",
    "        return predictions.flatten()\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 500\n",
    "output_size = 1\n",
    "\n",
    "mlp = MLP(input_size, hidden_size, output_size)\n",
    "mlp_regressor = PyTorchRegressor(model=mlp, epochs=5000, lr=0.1)\n",
    "mlp_regressor.fit(X_train, y_train)\n",
    "mlp_predictions = mlp_regressor.predict(X_train)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_train, mlp_predictions))\n",
    "scores = cross_val_score(mlp_regressor, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Scores: {rmse_scores}\")\n",
    "print(f\"Mean: {rmse_scores.mean()}\")\n",
    "print(f\"Standard Deviation: {rmse_scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note:\n",
    "\n",
    "1. The MLP has slightly better performance to the Random Forest regressor, and the overfitting is not as bad as random Forest.\n",
    "2. On GPU, the training time is the same as Random Forest, so this is a constrain.\n",
    "3. Hyperparameter optimization of this model was done manually (epochs and hidden layer sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamter optimization and Feature importance\n",
    "\n",
    "1. Do hyperparameter optimization for the Random Forest regressor\n",
    "2. Do Feature Importance to get an idea of the performance of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iox/anaconda3/envs/ai/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "405 fits failed out of a total of 1215.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/iox/anaconda3/envs/ai/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/iox/anaconda3/envs/ai/lib/python3.9/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/iox/anaconda3/envs/ai/lib/python3.9/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/iox/anaconda3/envs/ai/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/iox/anaconda3/envs/ai/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1052: UserWarning: One or more of the test scores are non-finite: [            nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan -2.76460684e+09\n",
      " -2.59355113e+09 -2.53731591e+09 -2.70894623e+09 -2.54981518e+09\n",
      " -2.51002331e+09 -2.72855119e+09 -2.58591709e+09 -2.55725873e+09\n",
      " -2.75337684e+09 -2.57645005e+09 -2.53850114e+09 -2.76123745e+09\n",
      " -2.58520536e+09 -2.55720531e+09 -2.65146084e+09 -2.62203996e+09\n",
      " -2.57209333e+09 -2.72165746e+09 -2.60445091e+09 -2.55956036e+09\n",
      " -2.76151358e+09 -2.63975328e+09 -2.59015105e+09 -2.72739343e+09\n",
      " -2.63427168e+09 -2.59611373e+09 -2.94267886e+09 -2.65013997e+09\n",
      " -2.61953176e+09 -2.82048235e+09 -2.71219843e+09 -2.58472755e+09\n",
      " -2.82390104e+09 -2.68296526e+09 -2.64340068e+09 -2.81561833e+09\n",
      " -2.67259148e+09 -2.61545666e+09 -2.83436654e+09 -2.63578912e+09\n",
      " -2.62826639e+09 -2.77471803e+09 -2.70790322e+09 -2.61566845e+09\n",
      " -2.82733143e+09 -2.72728241e+09 -2.64666286e+09 -2.85411091e+09\n",
      " -2.71597497e+09 -2.66414099e+09 -2.88444530e+09 -2.69734709e+09\n",
      " -2.66537977e+09             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      " -4.26666593e+09 -4.11496576e+09 -4.13314004e+09 -4.12930727e+09\n",
      " -4.11104179e+09 -4.13516160e+09 -4.18357265e+09 -4.16298239e+09\n",
      " -4.12445422e+09 -4.10950753e+09 -4.12023914e+09 -4.12675046e+09\n",
      " -4.16723308e+09 -4.12168241e+09 -4.06578836e+09 -4.24309425e+09\n",
      " -4.11903780e+09 -4.09905295e+09 -4.19475017e+09 -4.18036977e+09\n",
      " -4.06589396e+09 -4.16841056e+09 -4.13572826e+09 -4.11943477e+09\n",
      " -4.29032000e+09 -4.18384497e+09 -4.12298173e+09 -4.54869641e+09\n",
      " -4.29271750e+09 -4.28181669e+09 -4.42921460e+09 -4.32685684e+09\n",
      " -4.26863618e+09 -4.44913521e+09 -4.40194579e+09 -4.32729109e+09\n",
      " -4.42400000e+09 -4.35008964e+09 -4.31174062e+09 -4.45856526e+09\n",
      " -4.37854882e+09 -4.30668347e+09 -4.36809958e+09 -4.27587869e+09\n",
      " -4.31897480e+09 -4.40022688e+09 -4.39229047e+09 -4.29169324e+09\n",
      " -4.38209553e+09 -4.28224388e+09 -4.25569427e+09 -4.36797661e+09\n",
      " -4.37385255e+09 -4.31505352e+09             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan -2.97970188e+09 -2.86345210e+09 -2.79758341e+09\n",
      " -2.99304760e+09 -2.88446068e+09 -2.83644608e+09 -2.95325345e+09\n",
      " -2.85859735e+09 -2.81250521e+09 -2.94681597e+09 -2.85196600e+09\n",
      " -2.79653381e+09 -2.99033127e+09 -2.87202963e+09 -2.81493034e+09\n",
      " -2.92186476e+09 -2.85310066e+09 -2.83492369e+09 -2.94818215e+09\n",
      " -2.84701015e+09 -2.81445960e+09 -2.91971239e+09 -2.84239765e+09\n",
      " -2.82617921e+09 -3.01915762e+09 -2.87220471e+09 -2.85564229e+09\n",
      " -3.07180487e+09 -2.96741051e+09 -2.94204684e+09 -3.05190533e+09\n",
      " -2.95916396e+09 -2.90162197e+09 -3.09970538e+09 -2.96216833e+09\n",
      " -2.92299383e+09 -3.11690647e+09 -2.91954465e+09 -2.95629181e+09\n",
      " -3.09924502e+09 -2.96087601e+09 -2.89929853e+09 -3.03942941e+09\n",
      " -2.96921026e+09 -2.94147608e+09 -3.07591657e+09 -2.99526743e+09\n",
      " -2.93141310e+09 -3.09882335e+09 -2.94821621e+09 -2.93045940e+09\n",
      " -3.04567861e+09 -2.95737521e+09 -2.95065991e+09]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 30}\n",
      "Best RMSE Score: 50100.13279895036\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(estimator=forest_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = np.sqrt(-grid_search.best_score_)\n",
    "\n",
    "print('Best Parameters:', best_params)\n",
    "print('Best RMSE Score:', best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note:\n",
    "\n",
    "1. After doing grid search on the forest regressor, the best loss is around 50k, slightly worse than the MLP with manual tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.22873440019471142, 'median_income'),\n",
       " (0.10909431480194261, '<1H OCEAN'),\n",
       " (0.10744088904140649, 'is_capped'),\n",
       " (0.08787105390446648, 'age_of_households'),\n",
       " (0.0568932893333759, 'median_house_value'),\n",
       " (0.056617735895357686, 'population_per_household'),\n",
       " (0.05233745282798575, 'latitude'),\n",
       " (0.05158124681506645, 'bedrooms_per_room'),\n",
       " (0.04599225591097741, 'longitude'),\n",
       " (0.036920091891611176, 'population_density'),\n",
       " (0.02429162542348582, 'housing_median_age'),\n",
       " (0.019281229771860104, 'income_per_household'),\n",
       " (0.018462488770107508, 'total_rooms'),\n",
       " (0.01593517586685576, 'rooms_per_household'),\n",
       " (0.01332673513865565, 'income_per_room'),\n",
       " (0.011899464100642602, 'age_of_population'),\n",
       " (0.011775810667396325, 'income_per_bedroom'),\n",
       " (0.011187974658039827, 'households'),\n",
       " (0.010714615483946578, 'total_bedrooms'),\n",
       " (0.010362703092917685, 'population'),\n",
       " (0.008188268070129269, 'distance_to_high_value_area'),\n",
       " (0.007105449369700127, 'NEAR BAY'),\n",
       " (0.002397682757945338, 'ISLAND'),\n",
       " (0.0015702626319458, 'bedrooms_per_population'),\n",
       " (1.7783579470250168e-05, 'INLAND')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_attributes = ['bedrooms_per_room', \n",
    "                    'population_per_household', \n",
    "                    'rooms_per_household', \n",
    "                    'income_per_household', \n",
    "                    'income_per_bedroom', \n",
    "                    'income_per_room', \n",
    "                    'age_of_population', \n",
    "                    'age_of_households', \n",
    "                    'population_density', \n",
    "                    'bedrooms_per_population', \n",
    "                    'is_capped', \n",
    "                    'distance_to_high_value_area']\n",
    "\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "cat_encoder = pipeline.named_transformers_['cat']\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attributes + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note:\n",
    "\n",
    "1. distance_to_high_value_area is not a very important feature, and to get this feature a clustering model has to be trained, so this feature is dropped. The hypothesis is that '<1H OCEAN>' is highly correlated with distance_to_high_value_area and there is redundancy in these 2 features\n",
    "2. A threshold of 0.01 is going to be use to filter out the least important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "1. The MLP will be used for the e2e system. The reasons are:\n",
    "    * Better performance compared to RF (though limited optimization in the RF regressor was done)\n",
    "    * Scaling a NN is more straight forward (Data scaling, Model parallelism/sharding, distributed training/inference, quamtization, etc)\n",
    "    * As more data is available, the NN architecture can be arbitrarily more complex to fit the data.\n",
    "2. The feature engineering pipeline will be reworked due to the feature importance findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
