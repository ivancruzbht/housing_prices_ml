{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "1. This notebook implements 3 models\n",
    "    *   A lineal regression model as baseline\n",
    "    *   A tree-based model as potential candidate and also as a pipeline to do Feature importance\n",
    "    *   A MLP to use as the main regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test datasets preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28590/3990379248.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  high_value_homes['cluster'] = kmeans.fit_predict(high_value_homes[['latitude', 'longitude']])\n",
      "/tmp/ipykernel_28590/3990379248.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  high_value_homes['cluster'] = kmeans.fit_predict(high_value_homes[['latitude', 'longitude']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14448, 25)\n",
      "(6192, 25)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.centroids = None\n",
    "\n",
    "\n",
    "    def __compute_distance(self, point, centroid):\n",
    "        return np.sqrt((point[0] - centroid[0])**2 + (point[1] - centroid[1])**2)\n",
    "\n",
    "    def __distance_to_closest_centroid(self, row):\n",
    "        distances = [self.__compute_distance([row['latitude'], row['longitude']], centroid) for centroid in self.centroids]\n",
    "        return min(distances)\n",
    "\n",
    "    def __get_distance_to_high_value_area(self, X):\n",
    "        if self.centroids is None:\n",
    "            raise ValueError('Fit the model first before transforming the data')\n",
    "        \n",
    "        X['distance_to_high_value_area'] = X.apply(lambda row: self.__distance_to_closest_centroid(row), axis=1)\n",
    "        return X\n",
    "\n",
    "    # Fit the KMeans clustering model for the distance to high value area feature\n",
    "    def fit(self, X, y=None, num_clusters=2):\n",
    "        threshold = X['median_house_value'].quantile(0.80)\n",
    "        high_value_homes = X[X['median_house_value'] > threshold]\n",
    "\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=10)\n",
    "        high_value_homes['cluster'] = kmeans.fit_predict(high_value_homes[['latitude', 'longitude']])\n",
    "        self.centroids = kmeans.cluster_centers_\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['bedrooms_per_room'] = X['total_bedrooms'] / X['total_rooms']\n",
    "        X['population_per_household'] = X['population'] / X['households']\n",
    "        X['rooms_per_household'] = X['total_rooms'] / X['households']\n",
    "        X['income_per_household'] = X['median_income'] / X['households']\n",
    "        X['income_per_bedroom'] = X['median_income'] / X['total_bedrooms']\n",
    "        X['income_per_room'] = X['median_income'] / X['total_rooms']\n",
    "        X['age_of_population'] = X['housing_median_age'] / X['population']\n",
    "        X['age_of_households'] = X['housing_median_age'] / X['households']\n",
    "        X['population_density'] = X['population'] / X['total_rooms']\n",
    "        X['bedrooms_per_population'] = X['total_bedrooms'] / X['population']\n",
    "        X['is_capped'] = X['median_house_value'] == 500000\n",
    "        \n",
    "\n",
    "        self.__get_distance_to_high_value_area(X)\n",
    "        \n",
    "        return X\n",
    "\n",
    "# Custom transformer to drop a column    \n",
    "class DropColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.column_name in X.columns:\n",
    "            X = X.drop(columns=[self.column_name])\n",
    "        return X\n",
    "    \n",
    "housing_data = pd.read_csv(\"data/housing.csv\")\n",
    "train_set, test_set = train_test_split(housing_data, test_size=0.3, random_state=10)\n",
    "\n",
    "# Keep the original labels for training and testing\n",
    "y_train = train_set['median_house_value'].to_numpy()\n",
    "y_test = test_set['median_house_value'].to_numpy()\n",
    "\n",
    "num_attribs = train_set.columns.tolist()\n",
    "num_attribs.remove('ocean_proximity')\n",
    "cat_attribs = ['ocean_proximity']\n",
    "\n",
    "feature_eng = FeatureEngineering()\n",
    "feature_eng.fit(train_set)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('feature_eng', feature_eng),\n",
    "    ('drop_column', DropColumnTransformer('median_house_value')),\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attribs),\n",
    "    ('cat', OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(train_set)\n",
    "X_test = pipeline.transform(test_set)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test a linear regression model\n",
    "\n",
    "To evaluate:\n",
    "1. The Root Mean Squared Error is used for the training error \n",
    "2. N-fold cross validation with RMSE is used, with 10 folds for the validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 65512.19687815971\n",
      "Linear Regression RMSE Scores: [65491.56395045 62939.19496873 64423.00782818 65956.13642558\n",
      " 67187.4882399  66054.65590843 64872.79467416 79491.74134393\n",
      " 65726.6694338  70087.43228025]\n",
      "Linear Regression RMSE Mean: 67223.06850533944\n",
      "Linear Regression RMSE Standard Deviation: 4457.011346877264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "lin_predictions = lin_reg.predict(X_train)\n",
    "\n",
    "lin_rmse = np.sqrt(mean_squared_error(y_train, lin_predictions))\n",
    "lin_scores = cross_val_score(lin_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "\n",
    "print('Linear Regression RMSE:', lin_rmse)\n",
    "print('Linear Regression RMSE Scores:', lin_rmse_scores)\n",
    "print('Linear Regression RMSE Mean:', lin_rmse_scores.mean())\n",
    "print('Linear Regression RMSE Standard Deviation:', lin_rmse_scores.std())    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note:\n",
    "\n",
    "1. It seems that the model is underfitting as the error is around $67k\n",
    "2. We could try to:\n",
    "    * Use a more powerful model\n",
    "    * Add more features that give more signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.0\n",
      "Scores: [67342.37070237 64333.83323831 69251.34131217 65383.86508618\n",
      " 70360.5906065  66883.83561217 65856.91159433 66622.5999637\n",
      " 67069.65300694 65451.15859048]\n",
      "Mean: 66855.61597131658\n",
      "Standard Deviation: 1727.9332081551922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X_train, y_train)\n",
    "tree_pred = tree_reg.predict(X_train)\n",
    "\n",
    "tree_rmse = np.sqrt(mean_squared_error(y_train, tree_pred))\n",
    "scores = cross_val_score(tree_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(f\"RMSE: {tree_rmse}\")\n",
    "print(f\"Scores: {tree_rmse_scores}\")\n",
    "print(f\"Mean: {tree_rmse_scores.mean()}\")\n",
    "print(f\"Standard Deviation: {tree_rmse_scores.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note:\n",
    "\n",
    "1. Decision tree regressor is overfitting the data (Training RMSE = 0)\n",
    "2. Performance is a little bit worse than the lineal regressor model when cross-validated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 17607.54218024274\n",
      "Scores: [46340.26170842 44399.81293542 47564.20915831 46293.68816815\n",
      " 48362.21875891 48347.57464029 46158.84466152 48441.52319964\n",
      " 46878.14361248 49678.66839416]\n",
      "Mean: 47246.494523731606\n",
      "Standard Deviation: 1450.9680355512453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(X_train, y_train)\n",
    "forest_pred = forest_reg.predict(X_train)\n",
    "\n",
    "forest_rmse = np.sqrt(mean_squared_error(y_train, forest_pred))\n",
    "scores = cross_val_score(forest_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n",
    "forest_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(f\"RMSE: {forest_rmse}\")\n",
    "print(f\"Scores: {forest_rmse_scores}\")\n",
    "print(f\"Mean: {forest_rmse_scores.mean()}\")\n",
    "print(f\"Standard Deviation: {forest_rmse_scores.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note:\n",
    "\n",
    "1. The model stills overfit a lot but given that random forest is an ensemble mechanism, this helps to generalize and regularize so the overfitting is not as bad as the decision tree\n",
    "2. We could limit the 'max_depth', 'min_samples_split' and 'min_samples_leaf' to force the model to limit the complexity of the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layered Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'{device=}')\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x) \n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "\n",
    "# Custom wrapper class to use PyTorch model with scikit-learn cross-validation\n",
    "class PyTorchRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model, epochs=100, lr=0.01):\n",
    "        self.model = model.to(device)\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        self.model.train()\n",
    "        for _ in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X).cpu().numpy()\n",
    "        return predictions.flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 46813.458398005845\n",
      "Scores: [50216.31515483 55206.86811269 47869.48902031 48584.98297466\n",
      " 49532.24345142 53150.56898274 48171.97531494 56122.39500531\n",
      " 47208.99862308 98840.22129349]\n",
      "Mean: 55490.4057933461\n",
      "Standard Deviation: 14752.56330569965\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 500\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "dropout_rate = 0.3\n",
    "epochs = 2000\n",
    "\n",
    "mlp = MLP(input_size, hidden_size, output_size, dropout_rate=dropout_rate)\n",
    "mlp_regressor = PyTorchRegressor(model=mlp, epochs=epochs, lr=learning_rate) \n",
    "mlp_regressor.fit(X_train, y_train)\n",
    "mlp_predictions = mlp_regressor.predict(X_train)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_train, mlp_predictions))\n",
    "scores = cross_val_score(mlp_regressor, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Scores: {rmse_scores}\")\n",
    "print(f\"Mean: {rmse_scores.mean()}\")\n",
    "print(f\"Standard Deviation: {rmse_scores.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note:\n",
    "\n",
    "1. The MLP has slightly worse performance to the Random Forest regressor but the overfitting is not as bad as Random Forest.\n",
    "2. On GPU, the training time is the same as Random Forest, so this is a constrain.\n",
    "3. Hyperparameter optimization of this model was done manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamter optimization and Feature importance\n",
    "\n",
    "1. Do hyperparameter optimization for the Random Forest regressor\n",
    "2. Do Feature Importance to get an idea of the performance of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 30}\n",
      "Best RMSE Score: 50131.40484952308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(estimator=forest_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = np.sqrt(-grid_search.best_score_)\n",
    "\n",
    "print('Best Parameters:', best_params)\n",
    "print('Best RMSE Score:', best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note:\n",
    "\n",
    "1. After doing grid search on the forest regressor, the best loss is around 50k, slightly worse than the MLP with manual tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.22873440019471142, 'median_income'),\n",
       " (0.10909431480194261, '<1H OCEAN'),\n",
       " (0.10744088904140649, 'is_capped'),\n",
       " (0.08787105390446648, 'age_of_households'),\n",
       " (0.0568932893333759, 'median_house_value'),\n",
       " (0.056617735895357686, 'population_per_household'),\n",
       " (0.05233745282798575, 'latitude'),\n",
       " (0.05158124681506645, 'bedrooms_per_room'),\n",
       " (0.04599225591097741, 'longitude'),\n",
       " (0.036920091891611176, 'population_density'),\n",
       " (0.02429162542348582, 'housing_median_age'),\n",
       " (0.019281229771860104, 'income_per_household'),\n",
       " (0.018462488770107508, 'total_rooms'),\n",
       " (0.01593517586685576, 'rooms_per_household'),\n",
       " (0.01332673513865565, 'income_per_room'),\n",
       " (0.011899464100642602, 'age_of_population'),\n",
       " (0.011775810667396325, 'income_per_bedroom'),\n",
       " (0.011187974658039827, 'households'),\n",
       " (0.010714615483946578, 'total_bedrooms'),\n",
       " (0.010362703092917685, 'population'),\n",
       " (0.008188268070129269, 'distance_to_high_value_area'),\n",
       " (0.007105449369700127, 'NEAR BAY'),\n",
       " (0.002397682757945338, 'ISLAND'),\n",
       " (0.0015702626319458, 'bedrooms_per_population'),\n",
       " (1.7783579470250168e-05, 'INLAND')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_attributes = ['bedrooms_per_room', \n",
    "                    'population_per_household', \n",
    "                    'rooms_per_household', \n",
    "                    'income_per_household', \n",
    "                    'income_per_bedroom', \n",
    "                    'income_per_room', \n",
    "                    'age_of_population', \n",
    "                    'age_of_households', \n",
    "                    'population_density', \n",
    "                    'bedrooms_per_population', \n",
    "                    'is_capped', \n",
    "                    'distance_to_high_value_area']\n",
    "\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "cat_encoder = pipeline.named_transformers_['cat']\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attributes + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note:\n",
    "\n",
    "1. distance_to_high_value_area is not a very important feature, and to get this feature a clustering model has to be trained, so this feature is dropped. The hypothesis is that '<1H OCEAN>' is highly correlated with distance_to_high_value_area and there is redundancy in these 2 features\n",
    "2. A threshold of 0.01 is going to be use to filter out the least important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the MLP \n",
    "\n",
    "* New feature engineering pipeline to drop features and clustering functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14448, 20)\n",
      "(6192, 20)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['bedrooms_per_room'] = X['total_bedrooms'] / X['total_rooms']\n",
    "        X['is_capped'] = X['median_house_value'] == 500000\n",
    "        X['population_per_household'] = X['population'] / X['households']\n",
    "        X['rooms_per_household'] = X['total_rooms'] / X['households']\n",
    "        X['income_per_household'] = X['median_income'] / X['households']\n",
    "        X['income_per_bedroom'] = X['median_income'] / X['total_bedrooms']\n",
    "        X['income_per_room'] = X['median_income'] / X['total_rooms']\n",
    "        X['age_of_population'] = X['housing_median_age'] / X['population']\n",
    "        X['age_of_households'] = X['housing_median_age'] / X['households']\n",
    "        X['population_density'] = X['population'] / X['total_rooms']\n",
    "\n",
    "        # Commented out as feature importance is low\n",
    "        # X['bedrooms_per_population'] = X['total_bedrooms'] / X['population']\n",
    "        # self.__get_distance_to_high_value_area(X) \n",
    "        \n",
    "        return X\n",
    "\n",
    "# Custom transformer to drop a column    \n",
    "class DropColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if type(X) == pd.DataFrame:\n",
    "            if self.column in X.columns:\n",
    "                X = X.drop(columns=[self.column])\n",
    "        elif type(X) == np.ndarray:\n",
    "            X = np.delete(X, self.column, axis=1)\n",
    "        return X\n",
    "    \n",
    "housing_data = pd.read_csv(\"data/housing.csv\")\n",
    "train_set, test_set = train_test_split(housing_data, test_size=0.3, random_state=10)\n",
    "\n",
    "y_train = train_set['median_house_value'].to_numpy()\n",
    "y_test = test_set['median_house_value'].to_numpy()\n",
    "\n",
    "num_attribs = train_set.columns.tolist()\n",
    "num_attribs.remove('ocean_proximity')\n",
    "cat_attribs = ['ocean_proximity']\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('feature_eng', FeatureEngineering()),\n",
    "    ('drop_column', DropColumnTransformer('median_house_value')),\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder()),\n",
    "])\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attribs),\n",
    "    ('cat', cat_pipeline, cat_attribs),\n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(train_set)\n",
    "X_test = pipeline.transform(test_set)\n",
    "\n",
    "drop_column = DropColumnTransformer([19,21,22]) # Drop INLAND, ISLAND, NEAR BAY\n",
    "X_train = drop_column.transform(X_train)\n",
    "X_test = drop_column.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Training Dataset ------\n",
      "RMSE: 42168.42647211815\n",
      "Scores: [50189.63313682 50001.59119872 48548.9155991  51179.82613434\n",
      " 52919.27439262 51090.66989182 50340.13808272 63378.14632308\n",
      " 49437.95904561 52028.19715448]\n",
      "Mean: 51911.43509593046\n",
      "Standard Deviation: 4002.8670740595207\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for the feature engineering comparison (same as cell above for the original feature engineering pipeline)\n",
    "#input_size = X_train.shape[1]\n",
    "#hidden_size = 500\n",
    "#output_size = 1\n",
    "#learning_rate = 0.1\n",
    "#dropout_rate = 0.3\n",
    "#epochs = 2000\n",
    "\n",
    "\n",
    "# Tuned hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 1200\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "dropout_rate = 0.4\n",
    "epochs = 2000\n",
    "\n",
    "mlp = MLP(input_size, hidden_size, output_size, dropout_rate=dropout_rate)\n",
    "mlp_regressor = PyTorchRegressor(model=mlp, epochs=epochs, lr=learning_rate) \n",
    "mlp_regressor.fit(X_train, y_train)\n",
    "mlp_predictions = mlp_regressor.predict(X_train)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_train, mlp_predictions))\n",
    "scores = cross_val_score(mlp_regressor, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(\"------ Training Dataset ------\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Scores: {rmse_scores}\")\n",
    "print(f\"Mean: {rmse_scores.mean()}\")\n",
    "print(f\"Standard Deviation: {rmse_scores.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note:\n",
    "* The feature engineering pipeline was reworked to remove the least important features and the performace improved a little bit\n",
    "* Further hyperparameter optimization was done. The notebook shows the results of this. However if the claim of the feature engineering changes need to be ckecked, change the hyperparameters to the ones used in the original feature engineering pipeline (copy/pasted at the beginning of the cell above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the MLP with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Testing Dataset ------\n",
      "RMSE: 58383.24373039187\n"
     ]
    }
   ],
   "source": [
    "# Test dataset \n",
    "print(\"------ Testing Dataset ------\")\n",
    "mlp_predictions = mlp_regressor.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, mlp_predictions))\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "1. The MLP will be used for the e2e system. The reasons are:\n",
    "    * Better performance compared to RF (though limited optimization in the RF regressor was done)\n",
    "    * Scaling a NN is more straight forward (Data scaling, Model parallelism/sharding, distributed training/inference, quamtization, etc)\n",
    "    * As more data is available, the NN architecture can be arbitrarily more complex to fit the data.\n",
    "2. The MLP is slightly overfitting the training dataset because the test set loss is bigger. However the discrepancy is small and it is acceptable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
